{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76622e91",
   "metadata": {},
   "source": [
    "# Week 4 - Modern Digital Technologies in Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d03942",
   "metadata": {},
   "source": [
    "# Extracting the Data\n",
    "\n",
    "1. Text data collection using APIs\n",
    "2. Reading PDF file in Python\n",
    "3. Reading word (.docx) document\n",
    "4. Reading JSON object\n",
    "5. Reading HTML page and HTML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444a671",
   "metadata": {},
   "source": [
    "## 1. Collecting Data\n",
    "\n",
    "There are a lot of free APIs through which we can collect data and use it to solve problems.\n",
    "\n",
    "**Example**\n",
    "1. Free APIs like Twitter\n",
    "2. Wikipedia\n",
    "3. Government data (e.g. http://data.gov)\n",
    "4. Health care claim data (e.g. https://www.healthdata.gov/)\n",
    "\n",
    "## 1.1 Collecting Data from Tweets\n",
    "\n",
    "### Problem\n",
    "We want to collect text data using Twitter APIs.\n",
    "\n",
    "### Solution\n",
    "Twitter has a gigantic amount of data with a lot of value in it. Social media marketers are making their living from it. \n",
    "\n",
    "There is an enormous amount of tweets every day, and every tweet has some story to tell. When all of this data is collected and analyzed, it gives a tremendous amount of insights to a business about their company, product, service, etc. \n",
    "\n",
    "Let’s see how to pull the data in this section and then explore how to leverage it later.\n",
    "\n",
    "### How it works\n",
    "\n",
    "### Step 1.1 Log in to the Twitter developer portal\n",
    "Create your own app in the Twitter developer portal, and get the keys\n",
    "mentioned below. Once you have these credentials, you can start pulling\n",
    "data. \n",
    "\n",
    "Keys needed:\n",
    "\n",
    "- **Consumer key**: Key associated with the application (Twitter, Facebook, etc.).\n",
    "- **Consumer secret**: Password used to authenticate with the authentication server (Twitter, Facebook, etc.).\n",
    "- **Access token**: Key given to the client after successful authentication of above keys.\n",
    "- **Access token secret**: Password for the access key.\n",
    "\n",
    "### Step 1.2 Execute below query in Python\n",
    "Once all the credentials are in place, use the code below to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tweepy\n",
    "\n",
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4452a64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import tweepy\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# credentials\n",
    "consumer_key = \"*****************\"\n",
    "consumer_secret = \"*****************\"\n",
    "access_token = \"*****************\"\n",
    "access_token_secret = \"*****************\"\n",
    "\n",
    "# calling API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "query = \"ABC\"\n",
    "\n",
    "Tweets = api.search_tweets(query, count=10, lang='en', exclude='retweets', tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd924e",
   "metadata": {},
   "source": [
    "The query above will pull the top 10 tweets when the product `ABC` is\n",
    "searched. The API will pull English tweets since the language given is ‘en’\n",
    "and it will exclude retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2196c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0456a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdc2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c5aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1f4bdb",
   "metadata": {},
   "source": [
    "## 2. Collecting Data from PDFs\n",
    "\n",
    "Most of the time your data will be stored as PDF files. We need to extract text from these files and store it for further analysis.\n",
    "\n",
    "### Problem\n",
    "You want to read a PDF file.\n",
    "\n",
    "### Solution\n",
    "The simplest way to do this is by using the **PyPDF2** library.\n",
    "\n",
    "### How It Works\n",
    "Let’s follow the steps in this section to extract data from PDF files.\n",
    "\n",
    "### Step 2.1. Install and import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2dfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b8c8",
   "metadata": {},
   "source": [
    "### Step 2.2 Extracting text from PDF file\n",
    "\n",
    "**Note**: You can download any PDF file from the web and place it in the location where you are running this Jupyter notebook or Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5556950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pdf file object\n",
    "pdf = open(\"paper.pdf\", \"rb\")\n",
    "\n",
    "# create pdf reader object\n",
    "pdf_reader = PdfReader(pdf)\n",
    "\n",
    "# checking number of pages in a pdf file\n",
    "print(len(pdf_reader.pages), '\\n\\n')\n",
    "\n",
    "# creating a page object\n",
    "page = pdf_reader.pages[0]\n",
    "\n",
    "# finally extracting text from the page\n",
    "print(page.extract_text())\n",
    "\n",
    "# closing the pdf file\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbcc75",
   "metadata": {},
   "source": [
    "Please note that the function above doesn’t work for scanned PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362bcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51045d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d97c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14693fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ca5a50",
   "metadata": {},
   "source": [
    "## 3. Collecting Data from Word Files\n",
    "\n",
    "\n",
    "\n",
    "### Problem\n",
    "You want to read word (.docx) files.\n",
    "\n",
    "### Solution\n",
    "The simplest way to do this is by using the **docx** library.\n",
    "\n",
    "### How It Works\n",
    "Let’s follow the steps in this section to extract data from the Word file.\n",
    "\n",
    "### Step 3.1. Install and import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a309fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install docx\n",
    "\n",
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08adb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import docx\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a50de9",
   "metadata": {},
   "source": [
    "### Step 3-2 Extracting text from word file\n",
    "\n",
    "**Note**: You can download any Word file from the web and place it in the location where you are running this Jupyter notebook or Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word file object\n",
    "doc = open(\"word.docx\", \"rb\")\n",
    "\n",
    "# creating a reader object\n",
    "document = Document(doc)\n",
    "\n",
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "    \n",
    "# we can see the output by calling the docu\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce40b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6982b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e11d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc385f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e5cfe1",
   "metadata": {},
   "source": [
    "## 4. Collecting Data from JSON\n",
    "\n",
    "Reading a JSON file/object.\n",
    "\n",
    "### Problem\n",
    "We want to read a JSON file/object.\n",
    "\n",
    "### Solution\n",
    "The simplest way to do this is by using requests and the JSON library.\n",
    "\n",
    "### How It Works\n",
    "Let’s follow the steps in this section to extract data from the JSON.\n",
    "\n",
    "### Step 4.1. Install and import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf96f5",
   "metadata": {},
   "source": [
    "### Step 4.2. Extracting Text from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a785ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make a JSON file and store it\n",
    "\n",
    "data = '{\"success\": { \"total\": 1}, \"contents\": {\"quotes\": [ {\"quote\": \\\n",
    "\"Where there is ruin, there is hope for a treasure.\", \"length\": \"50\", \"author\": \"Rumi\", \\\n",
    "\"tags\": [\"failure\", \"inspire\", \"learning-from-failure\"], \"category\": \"inspire\", \"date\": \"2018-09-29\", \\\n",
    "\"permalink\": \"https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-\\\n",
    "ruin-there-is-hope-for-a-treasure\", \"title\": \"Inspiring Quote of the day\",\\\n",
    "\"background\": \"https://theysaidso.com/img/bgs/man_on_the_mountain.jpg\", \"id\": \"dPKsui4sQnQqgMnXHLKtfweF\"\\\n",
    "} ], \"copyright\": \"2017-19 theysaidso.com\"}}'\n",
    "\n",
    "temp2_file = json.dumps(data)\n",
    "\n",
    "with open(\"my_second_file.json\", \"w\") as file:\n",
    "    file.write(temp2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537de14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_second_file.json\", \"r\") as file:\n",
    "    temp = json.load(file)\n",
    "    \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"success\": { \"total\": 1}, \"contents\": {\"quotes\": [ {\"quote\": \\\n",
    "\"Where there is ruin, there is hope for a treasure.\", \"length\": \"50\", \"author\": \"Rumi\", \\\n",
    "\"tags\": [\"failure\", \"inspire\", \"learning-from-failure\"], \"category\": \"inspire\", \"date\": \"2018-09-29\", \\\n",
    "\"permalink\": \"https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-\\\n",
    "ruin-there-is-hope-for-a-treasure\", \"title\": \"Inspiring Quote of the day\",\\\n",
    "\"background\": \"https://theysaidso.com/img/bgs/man_on_the_mountain.jpg\", \"id\": \"dPKsui4sQnQqgMnXHLKtfweF\"\\\n",
    "} ], \"copyright\": \"2017-19 theysaidso.com\"}}\n",
    "\n",
    "temp = json.dumps(data, indent=3)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990aa210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract contents\n",
    "\n",
    "q = data['contents']['quotes'][0]\n",
    "\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only quote\n",
    "\n",
    "print(q['quote'], '\\n--', q['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a7954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a64152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e2898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79788cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e03d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fef78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3c4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a158a9f4",
   "metadata": {},
   "source": [
    "## 5. Collecting Data from HTML\n",
    "let us look at reading HTML pages.\n",
    "\n",
    "\n",
    "### Problem\n",
    "We want to parse/read HTML pages.\n",
    "\n",
    "### Solution\n",
    "The simplest way to do this is by using the **bs4** library.\n",
    "\n",
    "### How It Works\n",
    "Let’s follow the steps in this section to extract data from the web.\n",
    "\n",
    "### Step 5.1. Install and import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fe33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23893fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7e1f0",
   "metadata": {},
   "source": [
    "### Step 5.2. Fetch the HTML file\n",
    "Pick any website from the web that you want to extract. Let’s pick Wikipedia for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e89b3",
   "metadata": {},
   "source": [
    "### Step 5.3. Fetch the HTML file\n",
    "\n",
    "Now we get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc717f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "\n",
    "# pring few lines\n",
    "print(strhtm[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6c302",
   "metadata": {},
   "source": [
    "### Step 5.4. Extracting tag value\n",
    "\n",
    "We can extract a tag value from the first instance of the tag using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e884e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72711ce2",
   "metadata": {},
   "source": [
    "### Step 5.5. Extracting all instances of a particular tag\n",
    "\n",
    "Here we get all the instances of a tag that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d75f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in soup.find_all('a'):\n",
    "    print(x.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85d68e",
   "metadata": {},
   "source": [
    "### Step 5.6. Extracting all text of a particular tag\n",
    "\n",
    "Finally, we get the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e657c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in soup.find_all('p'):\n",
    "    print(x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf01d05",
   "metadata": {},
   "source": [
    "### Step 5.7. Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42790bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4149b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9788c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8cdb2",
   "metadata": {},
   "source": [
    "To get text out of HTML we will use a Python library called BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d409bac",
   "metadata": {},
   "source": [
    "### Step 5.8. Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b22bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "\n",
    "response = request.urlopen(url)\n",
    "\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[1:74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbddb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4e927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
