{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae901acd",
   "metadata": {},
   "source": [
    "# Week 7 - Modern Digital Technologies in Text Analysis\n",
    "\n",
    "# Advanced Natural Langage Processing\n",
    "\n",
    "In this seminar, we are going to cover various advanced NLP techniques and leverage machine learning algorithms to extract information from text data as well as some of the advanced NLP applications with the solution approach and implementation.\n",
    "\n",
    "1. Noun Phrase extraction\n",
    "2. Text similarity\n",
    "3. Parts of speech tagging\n",
    "4. Information extraction – NER – Entity recognition\n",
    "5. Topic modeling\n",
    "6. Text classification\n",
    "7. Sentiment analysis\n",
    "8. Word sense disambiguation\n",
    "9. Speech recognition and speech to text\n",
    "10. Text to speech\n",
    "11. Language detection and translation\n",
    "\n",
    "Before going further let's discuss and understand the NLP pipeline and life cycle first. There are so many concepts we are implementing in this course, and we might get overwhelmed by the content of it. To make it simpler and smoother, let’s see what is the flow that we need to follow for an NLP solution.\n",
    "\n",
    "For example, let’s consider customer sentiment analysis and prediction for a product or brand or service.\n",
    "\n",
    "- **Define the Problem:** Understand the customer sentiment across the products.\n",
    "- **Understand the depth and breadth of the problem:** Understand the customer/user sentiments across the product; why we are doing this? What is the business impact? Etc.\n",
    "\n",
    "- **Data requirement brainstorming:** Have a brainstorming activity to list out all possible data points.\n",
    "     - All the reviews from customers on e-commerce platforms like Amazon, Flipkart, etc.\n",
    "     - Emails sent by customers\n",
    "     - Warranty claim forms\n",
    "     - Survey data\n",
    "     - Call center conversations using speech to text\n",
    "     - Feedback forms\n",
    "     - Social media data like Twitter, Facebook, and LinkedIn\n",
    " \n",
    "- **Data collection:** We learned different techniques to collect the data eailier from different resources (Seminar: Extracting the Data). Based on the data and the problem, we might have to incorporate different data collection methods. In this case, we can use web scraping and Twitter APIs.\n",
    "\n",
    "- **Text Preprocessing:** We know that data won’t always be clean. We need to spend a significant amount of time to process it and extract insight out of it using different methods that we discussed earlier. (Seminar: Exploring and Processing Text Data)\n",
    "\n",
    "- **Text to feature:** As we discussed, texts are characters and machines will have a tough time understanding them. We have to convert them to features that machines and algorithms can understand using any of the methods we learned in the previous seminar.\n",
    "\n",
    "- **Machine learning/Deep learning:** Machine learning/ Deep learning is a part of an artificial intelligence umbrella that will make systems automatically learn patterns in the data without being programmed. Most of the NLP solutions are based on this, and since we converted text to features, we can leverage machine learning or deep learning algorithms to achieve the goals like text classification, natural language generation, etc.\n",
    "\n",
    "- **Insights and deployment:** There is absolutely no use or building NLP solutions without proper insights being communicated to the business. Always take time to connect the dots between model/analysis output and the business, thereby creating the maximum impact.\n",
    "\n",
    "\n",
    "## 1. Extracting Noun Phrases\n",
    "\n",
    "Let us extract a noun phrase from the text data (a sentence or the documents).\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to extract a noun phrase.\n",
    "\n",
    "### Solution\n",
    "\n",
    "Noun Phrase extraction is important when you want to analyze the “who” in a sentence. Let’s see an example below using TextBlob.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Execute the following code to extract noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e29ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract noun\n",
    "blob = TextBlob(\"John is learning natural language processing\")\n",
    "\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5baeea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d93f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763829b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a481f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbed739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e0a78a7",
   "metadata": {},
   "source": [
    "## 2. Finding Similarity Between Texts\n",
    "\n",
    "We are going to discuss how to find the similarity between two documents or text. There are many similarity metrics like Euclidian, cosine, Jaccard, etc. Applications of text similarity can be found in areas like spelling correction and data deduplication.\n",
    "\n",
    "Here are a few of the similarity measures:\n",
    "\n",
    "- **Cosine similarity:** Calculates the cosine of the angle between the two vectors.\n",
    "- **Jaccard similarity:** The score is calculated using the intersection or union of words.\n",
    "- **Jaccard Index:** (the number in both sets) / (the number in either set) * 100.\n",
    "- **Levenshtein distance:** Minimal number of insertions, deletions, and replacements required for transforming string “a” into string “b.”\n",
    "- **Hamming distance:** Number of positions with the same symbol in both strings. But it can be defined only for strings with equal length.\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to find the similarity between texts/documents.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The simplest way to do this is by using **cosine** similarity from the sklearn library.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Let’s follow the steps to compute the similarity score between text documents.\n",
    "\n",
    "### 2.1 Create/read the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cbbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\"I like NLP\",\n",
    "             \"I am exploring NLP\",\n",
    "             \"I am a beginner in NLP\",\n",
    "             \"I want to learn NLP\",\n",
    "             \"I like advanced NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b87e4",
   "metadata": {},
   "source": [
    "### Step 2-2 Find the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47235a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute tfidf : feature engineering (refer previous seminar)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity for first sentence with rest of the sentences\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1117b",
   "metadata": {},
   "source": [
    "If we clearly observe, the first sentence and last sentence have higher similarity compared to the rest of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf689cc",
   "metadata": {},
   "source": [
    "### Phonetic matching\n",
    "\n",
    "The next version of similarity checking is phonetic matching, which roughly matches the two words or sentences and also creates an alphanumeric string as an encoded version of the text or word. It is very useful for searching large text corpora, correcting spelling errors, and matching relevant names. **Soundex** and **Metaphone** are two main phonetic algorithms used for this purpose. The simplest way to do this is by using the fuzzy library.\n",
    "\n",
    "1. Install and import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df996c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "\n",
    "# !pip install Fuzzy\n",
    "# !pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71073269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzy\n",
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f978a",
   "metadata": {},
   "source": [
    "2. Generate the phonetic form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9859c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex1 = jellyfish.soundex('natural')\n",
    "soundex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80796767",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex2 = jellyfish.soundex('natuaral')\n",
    "soundex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex3 = jellyfish.soundex('language')\n",
    "soundex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex4 = jellyfish.soundex('processing')\n",
    "soundex4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669df41",
   "metadata": {},
   "source": [
    "Soundex is treating “natural” and “natuaral” as the same, and the phonetic code for both of the strings is “N364.” And for “language” and “processing,” it is “L522” and “P625” respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243739d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a798fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d58f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab601a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3afbfa27",
   "metadata": {},
   "source": [
    "## 3. Tagging Part of Speech\n",
    "\n",
    "Part of speech (POS) tagging is another crucial part of natural language processing that involves labeling the words with a part of speech such as noun, verb, adjective, etc. POS is the base for Named Entity Resolution, Sentiment Analysis, Question Answering, and Word Sense Disambiguation.\n",
    "\n",
    "### Problem\n",
    "\n",
    "Tagging the parts of speech for a sentence.\n",
    "\n",
    "### Solution\n",
    "\n",
    "There are 2 ways a tagger can be built.\n",
    "\n",
    "- Rule based - Rules created manually, which tag a word belonging to a particular POS.\n",
    "- Stochastic based - These algorithms capture the sequence of the words and tag the probability of the sequence using hidden Markov models.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "NLTK has the best POS tagging module. `nltk.pos_tag(word)` is the function that will generate the POS tagging for any given word. Use for loop and generate POS for all the words present in the document.\n",
    "\n",
    "### 3.1 Store the text in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love NLP and I will learn NLP in 2 month\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde226c",
   "metadata": {},
   "source": [
    "### 3.2 NLTK for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff78490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = sent_tokenize(text)\n",
    "\n",
    "# Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # POS-tagger.\n",
    "    tags = nltk.pos_tag(words)\n",
    "    \n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aca0c",
   "metadata": {},
   "source": [
    "Below are the short forms and explanation of POS tagging. The word `“love”` is VBP, which means verb, sing. present, non-3d take\n",
    "\n",
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” ... think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1)\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRP\\\\$ possessive pronoun my, his, hers\n",
    "- RB adverb very, silently\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "- TO to go ‘to’ the store\n",
    "- UH interjection\n",
    "- VB verb, base form take\n",
    "- VBD verb, past tense took\n",
    "- VBG verb, gerund/present participle taking\n",
    "- VBN verb, past participle taken\n",
    "- VBP verb, sing. present, non-3d take\n",
    "- VBZ verb, 3rd person sing. present takes\n",
    "- WDT wh-determiner which\n",
    "- WP wh-pronoun who, what\n",
    "- WP$ possessive wh-pronoun whose\n",
    "- WRB wh-adverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b88aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4648f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14f743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09166fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "087ab711",
   "metadata": {},
   "source": [
    "## 4. Extract Entities from Text\n",
    "\n",
    "We are going to discuss how to identify and extract entities from the text, called Named Entity Recognition. There are multiple libraries to perform this task like NLTK chunker, StanfordNER, SpaCy, opennlp, and NeuroNER; and there are a lot of APIs also like WatsonNLU, AlchemyAPI, NERD, Google Cloud NLP API, and many more.\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to identify and extract entities from the text.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The simplest way to do this is by using the **ne_chunk** from NLTK or **SpaCy**.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Let’s follow the steps in this section to perform NER.\n",
    "\n",
    "### 4.1 Read/create the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"John is studying at Stanford University in California\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef241256",
   "metadata": {},
   "source": [
    "### 4.2 Extract the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#NER\n",
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab3603",
   "metadata": {},
   "source": [
    "Here \"*John*\" is tagged as \"*PERSON*\"\n",
    "\n",
    "\"*Stanford*\" as \"*ORGANIZATION*\"\n",
    "\n",
    "\"*California*\" as \"*GPE*\". Geopolitical entity, i.e. countries, cities, states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2435c",
   "metadata": {},
   "source": [
    "### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f57e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca7bc7",
   "metadata": {},
   "source": [
    "According to the output, \n",
    "* Apple is an organization\n",
    "* 10000 is money\n",
    "* New York is place\n",
    "\n",
    "The results are accurate and can be used for any NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10a7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
